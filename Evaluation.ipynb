{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCGERDM6UKd_"
   },
   "source": [
    "# **Topic Model Evaluation**\n",
    "Here, you will find the code needed to run the experiments of the paper:\n",
    "\n",
    "*BERTopic: Neural topic modeling with a class-based TF-IDF procedure*.\n",
    "\n",
    "The package itself can be found [here](https://github.com/MaartenGr/BERTopic) and the repository for evaluation [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFwg78cJ0BFg"
   },
   "source": [
    "## **Installation**\n",
    "First, we need to install a few packages in order to run our experiments. Most of the packages are installed through the `tm_evaluation` package of which [OCTIS](https://github.com/MIND-Lab/OCTIS) is an important component. \n",
    "\n",
    "You can install the evaluation package with `pip install .` from the root. To additionally install CTM run `pip install .[ctm]`To install BERTopic, run `pip install bertopic==v0.9.4` after installing the base package or use `pip install .[bertopic]`. Top2Vec should be installed with `pip install top2vec==v1.0.26` after installing the base package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLesRC-17zN9"
   },
   "source": [
    "To run a faster version of LDAseq for dynamic topic modeling, we need to uninstall gensim and install a specific merge that allows for this speed-up. First, run `pip uninstall gensim -y`, then, run `pip install git+https://github.com/RaRe-Technologies/gensim.git@refs/pull/3172/merge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fisNVVa977e-"
   },
   "source": [
    "**NOTE**: After installing the above packages, make sure to restart the runtime otherwise you are likely to run into issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQnAdKJ-Do1S"
   },
   "source": [
    "#  1. **Data**\n",
    "Some of the data can be accessed through OCTIS, such as the `20NewsGroup` and `BBC_News` datasets. Other datasets, however, are downloaded and then run through OCTIS in order to be used in their pipeline. \n",
    "\n",
    "The datasets that we are going to be preparing are: \n",
    "* Trump's tweets\n",
    "* United Nations general debates between 2006 and 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nicho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "45290\n",
      "words filtering done\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"incels\").preprocess_octis(documents_path=\"incels.txt\", output_folder=\"incels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLvF2devjGZt"
   },
   "source": [
    "## Trump\n",
    "The data can be found here: https://www.thetrumparchive.com/faq\n",
    "\n",
    "Using our `DataLoader` we can prepare the documents and save them in an OCTIS-based format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-X1poWssOoQ",
    "outputId": "5c6b14ff-0ce3-43f0-d18e-fa3952cba31b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "53637\n",
      "words filtering done\n",
      "CPU times: total: 3min 57s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"trump\").prepare_docs(save=\"trump.txt\").preprocess_octis(output_folder=\"trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV-GDOko9CaC"
   },
   "source": [
    "Additionally, there isa DTM variant that creates 10 timesteps to be used in the dynamic topic modeling experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9P7qS9Qp56K"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"trump_dtm\").prepare_docs(save=\"trump_dtm.txt\").preprocess_octis(output_folder=\"trump_dtm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DFzHi8Tp_6Z"
   },
   "source": [
    "## United Nations\n",
    "\n",
    "The transcriptions of the United Nations (UN) general debates between 2006 and 2015. The data can be found here: https://runestone.academy/runestone/books/published/httlads/_static/un-general-debates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-exAptIjqBrO",
    "outputId": "56773d61-3810-4ef4-9f20-9ee339c08faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "69447\n",
      "words filtering done\n",
      "CPU times: user 22min, sys: 21.5 s, total: 22min 21s\n",
      "Wall time: 22min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader = DataLoader(dataset=\"un_dtm\").prepare_docs(save=\"un_dtm.txt\").preprocess_octis(output_folder=\"un_dtm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRJJvgHyjYa6"
   },
   "source": [
    "# 2. **Evaluation**\n",
    "After preparing our data, we can start evaluating the topic models as used in the experiments. OCTIS already has a number of models prepared that we can use directly as shown below. \n",
    "\n",
    "First, we specify what the dataset is and whether that was a custom dataset not found in OCTIS. To run our custom trump dataset, we run `dataset, custom = \"trump\", True`. In contrast, if we are to use the prepackaged 20NewsGroup dataset, we run `dataset, custom = \"20NewsGroup\", False` instead. \n",
    "\n",
    "The OCTIS datasets can be found [here](https://github.com/MIND-Lab/OCTIS#available-datasets). \n",
    "\n",
    "Second, we define a number of parameters to be used for the model. It uses the following format: \n",
    "\n",
    "`params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "\n",
    "were we define a number of topics to loop over and calculate the evluation metrics but also define a number of parameters used in the models. \n",
    "\n",
    "#### **Parameters**\n",
    "The parameters for LDA and NMF:\n",
    "\n",
    "\n",
    "```python\n",
    "params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "```\n",
    "\n",
    "The parameters for Top2Vec:\n",
    "\n",
    "```python\n",
    "params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "          \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}}\n",
    "```\n",
    "Note that the `min_cluster_size` is 15 for all datasets except BBC_News.\n",
    "\n",
    "The parameters for CTM:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "```\n",
    "\n",
    "The parameters for BERTopic:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "    \"min_topic_size\": 15,\n",
    "    \"verbose\": True\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the `min_topic_size` is 15 for all datasets except BBC_News. Note that we do not set a `embedding_model` here. We do this on purpose as we can generate the embeddings beforehand and pass those to BERTopic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOhlHj4SGush"
   },
   "source": [
    "## **OCTIS**\n",
    "Here, we can run the experiments for NMF and LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu1kVxBhA4iY"
   },
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOZl2Xoed_3a"
   },
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"trump\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"NMF_trump_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8s9zjC5A6JD"
   },
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uAZ_ckYBeFDY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\octis\\dataset\\dataset.py:330: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = df[df[1] == 'train'].append(df[df[1] == 'val'])\n",
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\octis\\dataset\\dataset.py:331: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(df[df[1] == 'test'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.03822239357559332\n",
      "diversity: 0.53\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.04654416942755496\n",
      "diversity: 0.47\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.026994572816327247\n",
      "diversity: 0.44333333333333336\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.02407425278256937\n",
      "diversity: 0.4525\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.043619316027619745\n",
      "diversity: 0.566\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\octis\\dataset\\dataset.py:330: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = df[df[1] == 'train'].append(df[df[1] == 'val'])\n",
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\octis\\dataset\\dataset.py:331: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(df[df[1] == 'test'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.024009496036820174\n",
      "diversity: 0.47\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.022318416614106516\n",
      "diversity: 0.435\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.03637574135670597\n",
      "diversity: 0.47\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.0516035350050972\n",
      "diversity: 0.5375\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.03938090473203538\n",
      "diversity: 0.518\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\octis\\dataset\\dataset.py:330: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = df[df[1] == 'train'].append(df[df[1] == 'val'])\n",
      "c:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\octis\\dataset\\dataset.py:331: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(df[df[1] == 'test'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.03246790542930718\n",
      "diversity: 0.48\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.04489353551954052\n",
      "diversity: 0.485\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.029764886318674997\n",
      "diversity: 0.51\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.03978900011277152\n",
      "diversity: 0.495\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.028831246428640815\n",
      "diversity: 0.51\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"incels\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"LDA_incels_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsRdU70OVsA4"
   },
   "source": [
    "## **CTM**\n",
    "Here, we use de CombinedTM of the Contextualized Topic Models:  https://github.com/MilaNLProc/contextualized-topic-models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zPUkcq9hXsV8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9625d1ff469649a4934a33294076d81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 113.24878258993618\tTime: 0:00:33.028306: : 100it [55:13, 33.13s/it]\n",
      "Sampling: [20/20]: : 20it [08:20, 25.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.09682970609323242\n",
      "diversity: 0.86\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 115.60714677379565\tTime: 0:00:33.306663: : 100it [55:28, 33.28s/it]\n",
      "Sampling: [20/20]: : 20it [08:24, 25.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.05243977003303975\n",
      "diversity: 0.81\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 118.66196548068847\tTime: 0:00:33.173135: : 100it [55:19, 33.19s/it]\n",
      "Sampling: [20/20]: : 20it [08:26, 25.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.030922979685721663\n",
      "diversity: 0.7033333333333334\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 122.02981500719235\tTime: 0:00:33.163312: : 100it [55:20, 33.20s/it]\n",
      "Sampling: [20/20]: : 20it [08:29, 25.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.01764962060192852\n",
      "diversity: 0.61\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 125.35346149520014\tTime: 0:00:33.271828: : 100it [55:28, 33.28s/it]\n",
      "Sampling: [20/20]: : 20it [08:30, 25.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.02201759693033918\n",
      "diversity: 0.566\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f03e94460cc4675b42d532c0566631a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [33/100]\t Seen Samples: [2359698/7150600]\tTrain Loss: 113.33262253799889\tTime: 0:00:33.336203: : 33it [18:14, 33.21s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8cff61c87deb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                       \u001b[0mcustom_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                       verbose=True)\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"CTM_incels_{i+1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, save)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_combo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             }\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomputation_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_tm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36m_train_tm_model\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqt_ctm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocess_ctm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_ctm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;31m# Train BERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36m_train_ctm\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mctm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_dataset_ctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mcomputation_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\contextualized_topic_models\\models\\ctm.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_dataset, validation_dataset, save_dir, verbose, patience, delta, n_samples)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;31m# train epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0msp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m             \u001b[0msamples_processed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\contextualized_topic_models\\models\\ctm.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0msamples_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_samples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m             \u001b[1;31m# batch_size x vocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mX_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_bow'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\contextualized_topic_models\\datasets\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;34m\"\"\"Return sample from dataset at index i.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_bow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mX_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_bow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mX_contextual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_contextual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"incels\", True\n",
    "    params = {\n",
    "        \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "        \"contextual_size\":768,\n",
    "        \"num_data_loader_workers\":0\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"CTM_incels_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deab6d17a7944b7c8b832633325a0802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 144.81998253151397\tTime: 0:00:33.917171: : 100it [55:50, 33.51s/it]\n",
      "Sampling: [20/20]: : 20it [08:37, 25.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.01926399125800415\n",
      "diversity: 0.321\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 152.24581625197973\tTime: 0:00:33.598527: : 100it [56:02, 33.62s/it]\n",
      "Sampling: [20/20]: : 20it [08:43, 26.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.012917291169221717\n",
      "diversity: 0.3064\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 162.345519608112\tTime: 0:00:33.721982: : 100it [56:10, 33.70s/it]\n",
      "Sampling: [20/20]: : 20it [08:48, 26.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.011227304923263858\n",
      "diversity: 0.25133333333333335\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 181.6485341169469\tTime: 0:00:33.930000: : 100it [56:34, 33.94s/it]\n",
      "Sampling: [20/20]: : 20it [08:54, 26.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.0124014507755403\n",
      "diversity: 0.2075\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 199.7473824926514\tTime: 0:00:34.429891: : 100it [56:58, 34.18s/it]\n",
      "Sampling: [20/20]: : 20it [09:04, 27.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.008336508369607568\n",
      "diversity: 0.1912\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f3f517969349e1a10fe4f917b2b004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 143.73808723765313\tTime: 0:00:33.758122: : 100it [56:00, 33.60s/it]\n",
      "Sampling: [20/20]: : 20it [08:38, 25.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.016081933324634475\n",
      "diversity: 0.355\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 152.831568555272\tTime: 0:00:33.764243: : 100it [56:10, 33.70s/it] \n",
      "Sampling: [20/20]: : 20it [08:43, 26.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.01694039142042814\n",
      "diversity: 0.304\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 162.5063383284707\tTime: 0:00:33.881380: : 100it [56:26, 33.87s/it]\n",
      "Sampling: [20/20]: : 20it [08:49, 26.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.011059868713475573\n",
      "diversity: 0.25266666666666665\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 181.32811751933951\tTime: 0:00:33.879039: : 100it [56:42, 34.03s/it]\n",
      "Sampling: [20/20]: : 20it [08:56, 26.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.008153982731317504\n",
      "diversity: 0.213\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [7150600/7150600]\tTrain Loss: 199.4580075803298\tTime: 0:00:34.244610: : 100it [57:07, 34.28s/it]\n",
      "Sampling: [20/20]: : 20it [09:03, 27.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.008644787721432222\n",
      "diversity: 0.1836\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    dataset, custom = \"incels\", True\n",
    "    params = {\n",
    "        \"n_components\": [100, 125, 150, 200, 250],\n",
    "        \"contextual_size\":768,\n",
    "        \"num_data_loader_workers\":0\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"CTM_CUSTOM\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"CTM_incels_big_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGhfpkBZGtRl"
   },
   "source": [
    "## **BERTopic**\n",
    "\n",
    "To speed up BERTopic, we can generate the embeddings before passing it to the `Trainer`. This way, the same embeddings do not have to be generated 5 times which speeds up evaluation quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XhFXGK8oxVpi"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"incels\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb1afGtJEuw0"
   },
   "source": [
    "As show above, we load in the `data` which the data loader and combine the tokens in each document to generate our training data. Then, we pass it to the sentence transformer model of our choice and generate the embeddings. \n",
    "\n",
    "Next, we pass these embeddings to the `bt_embeddings` parameter to speed up training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "II0a3WJP37V0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:22:24,039 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:22:31,698 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:22:58,788 - BERTopic - Reduced number of topics from 694 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.004570451738533479\n",
      "diversity: 0.4\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:24:23,757 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:24:29,512 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:24:56,735 - BERTopic - Reduced number of topics from 690 to 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.01535210597493621\n",
      "diversity: 0.375\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:26:23,552 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:26:29,398 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:26:57,715 - BERTopic - Reduced number of topics from 730 to 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.015255031960605145\n",
      "diversity: 0.43\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:28:29,695 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:28:35,610 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:29:02,886 - BERTopic - Reduced number of topics from 684 to 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.021258500329169597\n",
      "diversity: 0.4625\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:30:34,419 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:30:40,255 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:31:08,096 - BERTopic - Reduced number of topics from 720 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.025689675194642332\n",
      "diversity: 0.52\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:32:40,595 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:32:46,714 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:33:29,412 - BERTopic - Reduced number of topics from 719 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.007103936336285556\n",
      "diversity: 0.43\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:34:49,659 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:34:55,756 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:35:24,545 - BERTopic - Reduced number of topics from 736 to 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.009354392005119014\n",
      "diversity: 0.405\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:36:45,501 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:36:51,517 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:37:19,993 - BERTopic - Reduced number of topics from 712 to 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.014735427828483676\n",
      "diversity: 0.46\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:38:48,176 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:38:54,044 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:39:21,665 - BERTopic - Reduced number of topics from 703 to 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.019513620071159456\n",
      "diversity: 0.4925\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:40:51,407 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:40:57,126 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:41:24,959 - BERTopic - Reduced number of topics from 715 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.019151979268951427\n",
      "diversity: 0.488\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:43:01,682 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:43:07,579 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:43:35,901 - BERTopic - Reduced number of topics from 719 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.01040172161329305\n",
      "diversity: 0.38\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:45:00,643 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:45:06,538 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:45:34,947 - BERTopic - Reduced number of topics from 712 to 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.0006207277646848468\n",
      "diversity: 0.44\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:46:58,407 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:47:04,234 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:47:46,681 - BERTopic - Reduced number of topics from 701 to 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.006973075556417615\n",
      "diversity: 0.4633333333333333\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:49:10,732 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:49:16,605 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:49:45,183 - BERTopic - Reduced number of topics from 736 to 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.023247660928769717\n",
      "diversity: 0.5025\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 16:51:15,545 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-25 16:51:21,343 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-25 16:51:49,272 - BERTopic - Reduced number of topics from 710 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.032055600204222934\n",
      "diversity: 0.526\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"BERTopic_incels_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:11:55,098 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:12:02,060 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.14437168660825733\n",
      "diversity: 0.7679611650485437\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:16:37,171 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:16:42,878 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.10726969119614334\n",
      "diversity: 0.7492836676217765\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:18:54,902 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:19:01,273 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.04179228964749067\n",
      "diversity: 0.7196319018404908\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:20:57,059 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:21:04,558 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.0020698533028827935\n",
      "diversity: 0.6392523364485981\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:22:52,341 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:23:00,680 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.011842347466977217\n",
      "diversity: 0.580952380952381\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:24:42,064 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:24:48,753 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:25:30,810 - BERTopic - Reduced number of topics from 1196 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.006806845830887295\n",
      "diversity: 0.46\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:26:51,408 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:26:57,139 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:27:16,130 - BERTopic - Reduced number of topics from 362 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.013072839179449916\n",
      "diversity: 0.37\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:28:36,744 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:28:43,151 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:28:57,280 - BERTopic - Reduced number of topics from 174 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.017532417016635243\n",
      "diversity: 0.37\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:30:23,772 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:30:31,018 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:30:43,320 - BERTopic - Reduced number of topics from 105 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.00643639185295942\n",
      "diversity: 0.37\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:32:03,729 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:32:12,361 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:32:24,871 - BERTopic - Reduced number of topics from 89 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.010031836407403233\n",
      "diversity: 0.38\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:33:45,581 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:33:52,547 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:34:36,438 - BERTopic - Reduced number of topics from 1252 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.010065820441362262\n",
      "diversity: 0.464\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:35:59,654 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:36:05,442 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:36:24,537 - BERTopic - Reduced number of topics from 362 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.009541190895024969\n",
      "diversity: 0.412\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:37:48,153 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:37:54,504 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:38:08,167 - BERTopic - Reduced number of topics from 157 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.00010253445925954424\n",
      "diversity: 0.384\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:39:32,460 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:39:39,721 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:39:52,384 - BERTopic - Reduced number of topics from 113 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.005590126001773842\n",
      "diversity: 0.38\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:41:20,887 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:41:29,085 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:41:40,926 - BERTopic - Reduced number of topics from 84 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.0003194572448947324\n",
      "diversity: 0.348\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:43:03,968 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:43:11,056 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:43:54,306 - BERTopic - Reduced number of topics from 1219 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.02034090954512196\n",
      "diversity: 0.516\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:45:20,427 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:45:26,761 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:45:45,790 - BERTopic - Reduced number of topics from 376 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.026359669274147565\n",
      "diversity: 0.498\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:47:11,505 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:47:17,676 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:47:30,950 - BERTopic - Reduced number of topics from 164 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.024152977108041828\n",
      "diversity: 0.496\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:49:01,756 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:49:09,047 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:49:20,880 - BERTopic - Reduced number of topics from 107 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.018228457901122274\n",
      "diversity: 0.496\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:50:46,984 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:50:55,136 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:51:06,303 - BERTopic - Reduced number of topics from 80 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.020478484787817237\n",
      "diversity: 0.474\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:52:32,737 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:52:39,773 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:53:23,020 - BERTopic - Reduced number of topics from 1241 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.020308909957589057\n",
      "diversity: 0.5853333333333334\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:54:52,056 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:54:57,562 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:55:15,886 - BERTopic - Reduced number of topics from 368 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.020442260449947937\n",
      "diversity: 0.5266666666666666\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:56:44,881 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:56:50,958 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:57:03,633 - BERTopic - Reduced number of topics from 157 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.013544076446195681\n",
      "diversity: 0.54\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:58:37,635 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 13:58:44,762 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 13:58:56,144 - BERTopic - Reduced number of topics from 110 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.008251151238510836\n",
      "diversity: 0.5706666666666667\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:00:26,642 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:00:34,745 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:00:45,860 - BERTopic - Reduced number of topics from 87 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.015765179898670257\n",
      "diversity: 0.5773333333333334\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:02:20,237 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:02:27,065 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:03:08,993 - BERTopic - Reduced number of topics from 1204 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.010113043837290598\n",
      "diversity: 0.627\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:04:44,315 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:04:50,041 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:05:07,534 - BERTopic - Reduced number of topics from 355 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.01040257849717709\n",
      "diversity: 0.589\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:06:47,280 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:06:53,655 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:07:05,833 - BERTopic - Reduced number of topics from 152 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.0008632300351767871\n",
      "diversity: 0.62\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:08:47,366 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:08:54,573 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:09:05,572 - BERTopic - Reduced number of topics from 101 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.0003723860006885099\n",
      "diversity: 0.631\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:10:41,526 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:10:49,355 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:10:54,734 - BERTopic - Reduced number of topics from 86 to 86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.012256379332429674\n",
      "diversity: 0.5752941176470588\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:12:35,975 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:12:42,970 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.14438394935488627\n",
      "diversity: 0.7672025723472669\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:16:52,746 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:16:58,285 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.10654141779114833\n",
      "diversity: 0.7642011834319526\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:19:04,787 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:19:11,026 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.03656874426111189\n",
      "diversity: 0.7126666666666667\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:21:05,314 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:21:12,870 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.007035643061754532\n",
      "diversity: 0.6449541284403669\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:22:55,407 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:23:03,226 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.014930926342601361\n",
      "diversity: 0.5823529411764706\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:24:38,886 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:24:45,535 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:25:28,657 - BERTopic - Reduced number of topics from 1216 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.004166216864031016\n",
      "diversity: 0.43\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:26:46,974 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:26:52,613 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:27:11,576 - BERTopic - Reduced number of topics from 356 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.00514489189709368\n",
      "diversity: 0.42\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:28:32,767 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:28:38,936 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:28:52,727 - BERTopic - Reduced number of topics from 159 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.014036282402737058\n",
      "diversity: 0.36\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:30:12,633 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:30:19,843 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:30:32,375 - BERTopic - Reduced number of topics from 108 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.007717937980832447\n",
      "diversity: 0.37\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:31:56,783 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:32:05,041 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:32:16,991 - BERTopic - Reduced number of topics from 88 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.010939368400249002\n",
      "diversity: 0.37\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:33:37,559 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:33:44,307 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:34:27,403 - BERTopic - Reduced number of topics from 1198 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.0133288756906549\n",
      "diversity: 0.464\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:35:51,081 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:35:56,835 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:36:15,990 - BERTopic - Reduced number of topics from 368 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.0006844367318751346\n",
      "diversity: 0.428\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:37:43,078 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:37:49,223 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:38:02,989 - BERTopic - Reduced number of topics from 165 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.008375620740381657\n",
      "diversity: 0.428\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:39:25,969 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:39:33,093 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:39:45,412 - BERTopic - Reduced number of topics from 108 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.011473481657473684\n",
      "diversity: 0.408\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:41:08,119 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:41:16,098 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:41:27,744 - BERTopic - Reduced number of topics from 84 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.0036091746998922424\n",
      "diversity: 0.376\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:42:56,127 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:43:02,763 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:43:45,953 - BERTopic - Reduced number of topics from 1214 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.026221591525954654\n",
      "diversity: 0.534\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:45:12,226 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:45:18,047 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:45:36,679 - BERTopic - Reduced number of topics from 360 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.01933360919552401\n",
      "diversity: 0.454\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:47:02,147 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:47:08,321 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:47:21,766 - BERTopic - Reduced number of topics from 162 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.018753618557306884\n",
      "diversity: 0.48\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:48:52,322 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:48:59,239 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:49:11,273 - BERTopic - Reduced number of topics from 114 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.023171453735661034\n",
      "diversity: 0.494\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:50:42,776 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:50:50,848 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:51:02,293 - BERTopic - Reduced number of topics from 87 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.020933138058714523\n",
      "diversity: 0.464\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:52:32,654 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:52:39,509 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:53:22,135 - BERTopic - Reduced number of topics from 1221 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.018810279966924148\n",
      "diversity: 0.568\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:54:56,749 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:55:02,329 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:55:20,295 - BERTopic - Reduced number of topics from 353 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.017728005634066606\n",
      "diversity: 0.552\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:56:50,877 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:56:57,123 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:57:10,002 - BERTopic - Reduced number of topics from 156 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.020785323484659387\n",
      "diversity: 0.5546666666666666\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 14:58:38,874 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 14:58:46,113 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 14:58:57,740 - BERTopic - Reduced number of topics from 108 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.009734618921739137\n",
      "diversity: 0.5786666666666667\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:00:31,812 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:00:39,750 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:00:50,690 - BERTopic - Reduced number of topics from 85 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.016701869385311558\n",
      "diversity: 0.5653333333333334\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:02:20,006 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:02:26,843 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:03:10,115 - BERTopic - Reduced number of topics from 1259 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.002913690357061117\n",
      "diversity: 0.647\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:04:51,695 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:04:57,125 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:05:15,210 - BERTopic - Reduced number of topics from 371 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.0021977867974898387\n",
      "diversity: 0.606\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:06:55,288 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:07:01,621 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:07:14,243 - BERTopic - Reduced number of topics from 161 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.007121128058875267\n",
      "diversity: 0.627\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:08:50,208 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:08:57,235 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:09:08,286 - BERTopic - Reduced number of topics from 104 to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.0022316032195877867\n",
      "diversity: 0.648\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:10:48,772 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:10:56,857 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:11:02,245 - BERTopic - Reduced number of topics from 87 to 87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.011676044058734127\n",
      "diversity: 0.5930232558139535\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:12:41,798 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:12:48,449 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.14322951647009158\n",
      "diversity: 0.7668879668049793\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:17:06,894 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:17:12,576 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.10356173572174336\n",
      "diversity: 0.7595108695652174\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:19:28,828 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:19:35,124 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.03196943201921524\n",
      "diversity: 0.7\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:21:24,986 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:21:32,296 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.006826113784433472\n",
      "diversity: 0.6419047619047619\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:23:14,401 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:23:22,433 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.01076361169787531\n",
      "diversity: 0.5892857142857143\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:24:58,031 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:25:04,498 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:25:48,541 - BERTopic - Reduced number of topics from 1245 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.007773586993720549\n",
      "diversity: 0.43\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:27:08,200 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:27:13,865 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:27:33,075 - BERTopic - Reduced number of topics from 372 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.014706967355300743\n",
      "diversity: 0.37\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:28:54,023 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:29:00,305 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:29:14,214 - BERTopic - Reduced number of topics from 164 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.01578692458635863\n",
      "diversity: 0.36\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:30:34,556 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:30:41,767 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:30:54,252 - BERTopic - Reduced number of topics from 109 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.014683262493645276\n",
      "diversity: 0.37\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:32:14,626 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:32:22,478 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:32:34,371 - BERTopic - Reduced number of topics from 83 to 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.013584132519800219\n",
      "diversity: 0.35\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:33:54,819 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:34:01,407 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:34:44,353 - BERTopic - Reduced number of topics from 1202 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.013298155646542527\n",
      "diversity: 0.436\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:36:07,380 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:36:12,894 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:36:31,416 - BERTopic - Reduced number of topics from 339 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.008334856565224409\n",
      "diversity: 0.396\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:37:53,719 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:37:59,781 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:38:13,626 - BERTopic - Reduced number of topics from 167 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.003445588515009513\n",
      "diversity: 0.364\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:39:37,222 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:39:44,240 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:39:56,499 - BERTopic - Reduced number of topics from 107 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.009628523263208845\n",
      "diversity: 0.4\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:41:19,224 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:41:27,159 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:41:38,711 - BERTopic - Reduced number of topics from 83 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.0022253443483912254\n",
      "diversity: 0.364\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:43:00,980 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:43:07,723 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:43:51,417 - BERTopic - Reduced number of topics from 1217 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.02201266078329074\n",
      "diversity: 0.528\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:45:18,642 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:45:24,389 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:45:43,321 - BERTopic - Reduced number of topics from 364 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.02486908216462185\n",
      "diversity: 0.498\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:47:10,047 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:47:16,497 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:47:30,079 - BERTopic - Reduced number of topics from 168 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.024535662048674314\n",
      "diversity: 0.472\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:48:56,532 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:49:03,869 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:49:15,790 - BERTopic - Reduced number of topics from 105 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.01771468883047794\n",
      "diversity: 0.474\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:50:42,832 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:50:51,645 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:51:02,935 - BERTopic - Reduced number of topics from 85 to 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.02161410581087038\n",
      "diversity: 0.484\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:52:31,438 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:52:37,978 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:53:20,988 - BERTopic - Reduced number of topics from 1205 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.01882927170574085\n",
      "diversity: 0.6\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:54:50,481 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:54:56,138 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:55:14,636 - BERTopic - Reduced number of topics from 367 to 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: 0.020441106342870302\n",
      "diversity: 0.5293333333333333\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:56:44,898 - BERTopic - Reduced dimensionality with UMAP\n",
      "2023-03-26 15:56:51,662 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2023-03-26 15:57:04,948 - BERTopic - Reduced number of topics from 168 to 76\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-33c169bc4f98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                       \u001b[0mcustom_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                       verbose=True)\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"BERTopic_incels_extended_{i+1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, save)\u001b[0m\n\u001b[0;32m    163\u001b[0m             }\n\u001b[0;32m    164\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomputation_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_tm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[1;31m# Update results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, output_tm)\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m                     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\octis\\evaluation_metrics\\coherence_metrics.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, model_output)\u001b[0m\n\u001b[0;32m     62\u001b[0m             npmi = CoherenceModel(topics=topics, texts=self._texts, dictionary=self._dictionary,\n\u001b[0;32m     63\u001b[0m                                   coherence=self.measure, processes=1, topn=self.topk)\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnpmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mconfirmed_measures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate_measures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfirmed_measures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[1;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[0msegmented_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_support\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_support\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36mestimate_probabilities\u001b[1;34m(self, segmented_topics)\u001b[0m\n\u001b[0;32m    545\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\topic_coherence\\probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[1;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"using %s to estimate probabilities from sliding windows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py\u001b[0m in \u001b[0;36maccumulate\u001b[1;34m(self, texts, window_size)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_co_occurrences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_co_occurrences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_symmetrize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py\u001b[0m in \u001b[0;36mpartial_accumulate\u001b[1;34m(self, texts, window_size)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordOccurrenceAccumulator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcombo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_co_occurrences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcombo\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py\u001b[0m in \u001b[0;36maccumulate\u001b[1;34m(self, texts, window_size)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvirtual_document\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwindows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvirtual_document\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_docs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py\u001b[0m in \u001b[0;36manalyze_text\u001b[1;34m(self, window, doc_num)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0manalyze_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slide_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uniq_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# to exclude none token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"nr_topics\": [None, 10, 25, 50, 75, 100],\n",
    "        \"min_topic_size\": [10, 25, 50, 75, 100],\n",
    "        \"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"BERTopic_incels_extended_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpyBVa1Ka9Hw"
   },
   "source": [
    "## **Top2Vec**\n",
    "Aside from its Doc2Vec backend, we also want to explore its performance using the `\"all-mpnet-base-v2\"` SBERT model as that was used in BERTopic. To do so, we make a very slight change to the core code of Top2Vec, namely replacing all instances of `\"\"distiluse-base-multilingual-cased\"` with `\"all-mpnet-base-v2\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Pc-PXbV205R3"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "import umap\n",
    "import hdbscan\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import dbscan\n",
    "import tempfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "try:\n",
    "    import hnswlib\n",
    "\n",
    "    _HAVE_HNSWLIB = True\n",
    "except ImportError:\n",
    "    _HAVE_HNSWLIB = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text\n",
    "\n",
    "    _HAVE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _HAVE_TENSORFLOW = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    _HAVE_TORCH = True\n",
    "except ImportError:\n",
    "    _HAVE_TORCH = False\n",
    "\n",
    "logger = logging.getLogger('top2vec')\n",
    "logger.setLevel(logging.WARNING)\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(sh)\n",
    "\n",
    "\n",
    "def default_tokenizer(doc):\n",
    "    \"\"\"Tokenize documents for training and remove too long/short words\"\"\"\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True)\n",
    "\n",
    "\n",
    "class Top2VecNew(Top2Vec):\n",
    "    \"\"\"\n",
    "    Top2Vec\n",
    "    Creates jointly embedded topic, document and word vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_model: string\n",
    "        This will determine which model is used to generate the document and\n",
    "        word embeddings. The valid string options are:\n",
    "            * doc2vec\n",
    "            * universal-sentence-encoder\n",
    "            * universal-sentence-encoder-multilingual\n",
    "            * distiluse-base-multilingual-cased\n",
    "        For large data sets and data sets with very unique vocabulary doc2vec\n",
    "        could produce better results. This will train a doc2vec model from\n",
    "        scratch. This method is language agnostic. However multiple languages\n",
    "        will not be aligned.\n",
    "        Using the universal sentence encoder options will be much faster since\n",
    "        those are pre-trained and efficient models. The universal sentence\n",
    "        encoder options are suggested for smaller data sets. They are also\n",
    "        good options for large data sets that are in English or in languages\n",
    "        covered by the multilingual model. It is also suggested for data sets\n",
    "        that are multilingual.\n",
    "        For more information on universal-sentence-encoder visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "        For more information on universal-sentence-encoder-multilingual visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
    "        The distiluse-base-multilingual-cased pre-trained sentence transformer\n",
    "        is suggested for multilingual datasets and languages that are not\n",
    "        covered by the multilingual universal sentence encoder. The\n",
    "        transformer is significantly slower than the universal sentence\n",
    "        encoder options.\n",
    "        For more informati ond istiluse-base-multilingual-cased visit:\n",
    "        https://www.sbert.net/docs/pretrained_models.html\n",
    "    embedding_model_path: string (Optional)\n",
    "        Pre-trained embedding models will be downloaded automatically by\n",
    "        default. However they can also be uploaded from a file that is in the\n",
    "        location of embedding_model_path.\n",
    "        Warning: the model at embedding_model_path must match the\n",
    "        embedding_model parameter type.\n",
    "    documents: List of str\n",
    "        Input corpus, should be a list of strings.\n",
    "    min_count: int (Optional, default 50)\n",
    "        Ignores all words with total frequency lower than this. For smaller\n",
    "        corpora a smaller min_count will be necessary.\n",
    "    speed: string (Optional, default 'learn')\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        It will determine how fast the model takes to train. The\n",
    "        fast-learn option is the fastest and will generate the lowest quality\n",
    "        vectors. The learn option will learn better quality vectors but take\n",
    "        a longer time to train. The deep-learn option will learn the best\n",
    "        quality vectors but will take significant time to train. The valid\n",
    "        string speed options are:\n",
    "        \n",
    "            * fast-learn\n",
    "            * learn\n",
    "            * deep-learn\n",
    "    use_corpus_file: bool (Optional, default False)\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        Setting use_corpus_file to True can sometimes provide speedup for\n",
    "        large datasets when multiple worker threads are available. Documents\n",
    "        are still passed to the model as a list of str, the model will create\n",
    "        a temporary corpus file for training.\n",
    "    document_ids: List of str, int (Optional)\n",
    "        A unique value per document that will be used for referring to\n",
    "        documents in search results. If ids are not given to the model, the\n",
    "        index of each document in the original corpus will become the id.\n",
    "    keep_documents: bool (Optional, default True)\n",
    "        If set to False documents will only be used for training and not saved\n",
    "        as part of the model. This will reduce model size. When using search\n",
    "        functions only document ids will be returned, not the actual\n",
    "        documents.\n",
    "    workers: int (Optional)\n",
    "        The amount of worker threads to be used in training the model. Larger\n",
    "        amount will lead to faster training.\n",
    "    \n",
    "    tokenizer: callable (Optional, default None)\n",
    "        Override the default tokenization method. If None then\n",
    "        gensim.utils.simple_preprocess will be used.\n",
    "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
    "        If using an embedding model other than doc2vec, use the model's\n",
    "        tokenizer for document embedding. If set to True the tokenizer, either\n",
    "        default or passed callable will be used to tokenize the text to\n",
    "        extract the vocabulary for word embedding.\n",
    "    umap_args: dict (Optional, default None)\n",
    "        Pass custom arguments to UMAP.\n",
    "    hdbscan_args: dict (Optional, default None)\n",
    "        Pass custom arguments to HDBSCAN.\n",
    "    \n",
    "    verbose: bool (Optional, default True)\n",
    "        Whether to print status data during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 documents,\n",
    "                 min_count=50,\n",
    "                 embedding_model='doc2vec',\n",
    "                 embedding_model_path=None,\n",
    "                 speed='learn',\n",
    "                 use_corpus_file=False,\n",
    "                 document_ids=None,\n",
    "                 keep_documents=True,\n",
    "                 workers=None,\n",
    "                 tokenizer=None,\n",
    "                 use_embedding_model_tokenizer=False,\n",
    "                 umap_args=None,\n",
    "                 hdbscan_args=None,\n",
    "                 verbose=True\n",
    "                 ):\n",
    "\n",
    "        if verbose:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "            self.verbose = True\n",
    "        else:\n",
    "            logger.setLevel(logging.WARNING)\n",
    "            self.verbose = False\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = default_tokenizer\n",
    "\n",
    "        # validate documents\n",
    "        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if keep_documents:\n",
    "            self.documents = np.array(documents, dtype=\"object\")\n",
    "        else:\n",
    "            self.documents = None\n",
    "\n",
    "        # validate document ids\n",
    "        if document_ids is not None:\n",
    "            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n",
    "                raise ValueError(\"Documents ids need to be a list of str or int\")\n",
    "\n",
    "            if len(documents) != len(document_ids):\n",
    "                raise ValueError(\"Document ids need to match number of documents\")\n",
    "            elif len(document_ids) != len(set(document_ids)):\n",
    "                raise ValueError(\"Document ids need to be unique\")\n",
    "\n",
    "            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.str_\n",
    "            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.int_\n",
    "            else:\n",
    "                raise ValueError(\"Document ids need to be str or int\")\n",
    "\n",
    "            self.document_ids_provided = True\n",
    "            self.document_ids = np.array(document_ids)\n",
    "            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n",
    "        else:\n",
    "            self.document_ids_provided = False\n",
    "            self.document_ids = np.array(range(0, len(documents)))\n",
    "            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n",
    "            self.doc_id_type = np.int_\n",
    "\n",
    "        acceptable_embedding_models = [\"universal-sentence-encoder-multilingual\",\n",
    "                                       \"universal-sentence-encoder\",\n",
    "                                       \"all-mpnet-base-v2\"]\n",
    "\n",
    "        self.embedding_model_path = embedding_model_path\n",
    "\n",
    "        if embedding_model == 'doc2vec':\n",
    "\n",
    "            # validate training inputs\n",
    "            if speed == \"fast-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 40\n",
    "            elif speed == \"learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 40\n",
    "            elif speed == \"deep-learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 400\n",
    "            elif speed == \"test-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 1\n",
    "            else:\n",
    "                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n",
    "\n",
    "            if workers is None:\n",
    "                pass\n",
    "            elif isinstance(workers, int):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"workers needs to be an int\")\n",
    "\n",
    "            doc2vec_args = {\"vector_size\": 300,\n",
    "                            \"min_count\": min_count,\n",
    "                            \"window\": 15,\n",
    "                            \"sample\": 1e-5,\n",
    "                            \"negative\": negative,\n",
    "                            \"hs\": hs,\n",
    "                            \"epochs\": epochs,\n",
    "                            \"dm\": 0,\n",
    "                            \"dbow_words\": 1}\n",
    "\n",
    "            if workers is not None:\n",
    "                doc2vec_args[\"workers\"] = workers\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            if use_corpus_file:\n",
    "                processed = [' '.join(tokenizer(doc)) for doc in documents]\n",
    "                lines = \"\\n\".join(processed)\n",
    "                temp = tempfile.NamedTemporaryFile(mode='w+t')\n",
    "                temp.write(lines)\n",
    "                doc2vec_args[\"corpus_file\"] = temp.name\n",
    "\n",
    "\n",
    "            else:\n",
    "                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n",
    "                doc2vec_args[\"documents\"] = train_corpus\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "            self.embedding_model = 'doc2vec'\n",
    "            self.model = Doc2Vec(**doc2vec_args)\n",
    "\n",
    "            if use_corpus_file:\n",
    "                temp.close()\n",
    "\n",
    "        elif embedding_model in acceptable_embedding_models:\n",
    "\n",
    "            self.embed = None\n",
    "            self.embedding_model = embedding_model\n",
    "\n",
    "            self._check_import_status()\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            # preprocess documents\n",
    "            tokenized_corpus = [tokenizer(doc) for doc in documents]\n",
    "\n",
    "            def return_doc(doc):\n",
    "                return doc\n",
    "\n",
    "            # preprocess vocabulary\n",
    "            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n",
    "            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n",
    "            words = vectorizer.get_feature_names()\n",
    "            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n",
    "            vocab_inds = np.where(word_counts > min_count)[0]\n",
    "\n",
    "            if len(vocab_inds) == 0:\n",
    "                raise ValueError(f\"A min_count of {min_count} results in \"\n",
    "                                 f\"all words being ignored, choose a lower value.\")\n",
    "            self.vocab = [words[ind] for ind in vocab_inds]\n",
    "\n",
    "            self._check_model_status()\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "\n",
    "            # embed words\n",
    "            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n",
    "\n",
    "            # embed documents\n",
    "            if use_embedding_model_tokenizer:\n",
    "                self.document_vectors = self._embed_documents(documents)\n",
    "            else:\n",
    "                train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
    "                self.document_vectors = self._embed_documents(train_corpus)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n",
    "\n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "\n",
    "        if umap_args is None:\n",
    "            umap_args = {'n_neighbors': 15,\n",
    "                         'n_components': 5,\n",
    "                         'metric': 'cosine'}\n",
    "\n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "\n",
    "        if hdbscan_args is None:\n",
    "            hdbscan_args = {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "\n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "\n",
    "        # assign documents to topic\n",
    "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
    "                                                                      self._get_document_vectors())\n",
    "\n",
    "        # calculate topic sizes\n",
    "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
    "\n",
    "        # re-order topics\n",
    "        self._reorder_topics(hierarchy=False)\n",
    "\n",
    "        # initialize variables for hierarchical topic reduction\n",
    "        self.topic_vectors_reduced = None\n",
    "        self.doc_top_reduced = None\n",
    "        self.doc_dist_reduced = None\n",
    "        self.topic_sizes_reduced = None\n",
    "        self.topic_words_reduced = None\n",
    "        self.topic_word_scores_reduced = None\n",
    "        self.hierarchy = None\n",
    "\n",
    "        # initialize document indexing variables\n",
    "        self.document_index = None\n",
    "        self.serialized_document_index = None\n",
    "        self.documents_indexed = False\n",
    "        self.index_id2doc_id = None\n",
    "        self.doc_id2index_id = None\n",
    "\n",
    "        # initialize word indexing variables\n",
    "        self.word_index = None\n",
    "        self.serialized_word_index = None\n",
    "        self.words_indexed = False\n",
    "\n",
    "    def _check_import_status(self):\n",
    "        if self.embedding_model != 'all-mpnet-base-v2':\n",
    "            if not _HAVE_TENSORFLOW:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
    "        else:\n",
    "            if not _HAVE_TORCH:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install torch sentence_transformers\")\n",
    "\n",
    "    def _check_model_status(self):\n",
    "        if self.embed is None:\n",
    "            if self.verbose is False:\n",
    "                logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            if self.embedding_model != \"all-mpnet-base-v2\":\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    if self.embedding_model == \"universal-sentence-encoder-multilingual\":\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "                    else:\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                self.embed = hub.load(module)\n",
    "\n",
    "            else:\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    module = 'all-mpnet-base-v2'\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                model = SentenceTransformer(module)\n",
    "                self.embed = model.encode\n",
    "\n",
    "        if self.verbose is False:\n",
    "            logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz16gmSjHcs6"
   },
   "source": [
    "We can then use this `Top2VecNew` class to run our experiments including the `\"all-mpnet-base-v2\"` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NdPp3Xsda_SJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 16:21:12,903 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-03-26 16:21:23,010 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 16:25:40,573 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 16:26:31,750 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 16:26:37,855 - top2vec - INFO - Finding topics\n",
      "2023-03-26 16:41:29,620 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.1600470102094905\n",
      "diversity: 0.96\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 16:41:40,293 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 16:46:00,052 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 16:46:36,202 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 16:46:42,334 - top2vec - INFO - Finding topics\n",
      "2023-03-26 17:01:18,016 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.23205979683152655\n",
      "diversity: 0.89\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 17:01:28,173 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 17:05:41,836 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 17:06:12,553 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 17:06:18,664 - top2vec - INFO - Finding topics\n",
      "2023-03-26 17:20:37,086 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.2064239659960237\n",
      "diversity: 0.91\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 17:20:47,105 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 17:25:01,384 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 17:25:32,439 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 17:25:38,471 - top2vec - INFO - Finding topics\n",
      "2023-03-26 17:39:49,876 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.26100336239409827\n",
      "diversity: 0.82\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 17:39:59,644 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 17:44:14,629 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 17:44:50,732 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 17:44:56,856 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.25112036852653696\n",
      "diversity: 0.84\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 17:59:14,086 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-03-26 17:59:24,116 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 18:03:38,705 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 18:04:14,901 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 18:04:21,312 - top2vec - INFO - Finding topics\n",
      "2023-03-26 18:19:04,696 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.14817388226138214\n",
      "diversity: 0.96\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:19:14,570 - top2vec - INFO - Creating joint document/word embedding\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a89df1ada10e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                       \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                       verbose=True)\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"Top2Vec_incels_{i+1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, save)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_combo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             }\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomputation_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_tm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36m_train_tm_model\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;31m# Train Top2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Top2Vec\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_top2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;31m# Train LDAseq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36m_train_top2vec\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnr_topics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Creating joint document/word embedding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'doc2vec'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdoc2vec_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_normed_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         super(Doc2Vec, self).__init__(\n\u001b[0m\u001b[0;32m    297\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             self.train(\n\u001b[0m\u001b[0;32m    428\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_doctags'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         super(Doc2Vec, self).train(\n\u001b[0m\u001b[0;32m    517\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[0;32m   1071\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1429\u001b[0m             \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1431\u001b[1;33m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[0;32m   1432\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1286\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1287\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"incels\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                    #   custom_model=Top2VecNew,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"Top2Vec_incels_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:22:08,810 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-03-26 18:22:20,297 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2023-03-26 18:22:20,925 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 18:23:12,618 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 18:23:43,158 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 18:23:48,713 - top2vec - INFO - Finding topics\n",
      "2023-03-26 18:29:26,002 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.2779866971094175\n",
      "diversity: 0.89\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:29:37,532 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2023-03-26 18:29:37,787 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 18:30:26,670 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 18:31:01,416 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 18:31:06,681 - top2vec - INFO - Finding topics\n",
      "2023-03-26 18:36:30,768 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.21193039295405142\n",
      "diversity: 0.86\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:36:42,352 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2023-03-26 18:36:42,609 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 18:37:31,746 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 18:38:06,796 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 18:38:12,263 - top2vec - INFO - Finding topics\n",
      "2023-03-26 18:43:33,542 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.2347784383154207\n",
      "diversity: 0.7666666666666667\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:43:45,025 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2023-03-26 18:43:45,281 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 18:44:34,392 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 18:45:04,693 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 18:45:10,074 - top2vec - INFO - Finding topics\n",
      "2023-03-26 18:50:39,909 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.22414189284090452\n",
      "diversity: 0.745\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:50:51,415 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2023-03-26 18:50:51,685 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-26 18:51:40,955 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-26 18:52:16,055 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-26 18:52:21,405 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.22570013110888387\n",
      "diversity: 0.718\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:57:51,785 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-03-26 18:58:03,812 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2023-03-26 18:58:04,066 - top2vec - INFO - Creating joint document/word embedding\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e71267a166a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                       \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                       verbose=True)\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"Top2Vec_incels_MiniLM_{i+1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, save)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_combo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             }\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomputation_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_tm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36m_train_tm_model\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;31m# Train Top2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Top2Vec\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_top2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;31m# Train LDAseq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\evaluation\\evaluation.py\u001b[0m in \u001b[0;36m_train_top2vec\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnr_topics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[0;32m    659\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m                     \u001b[0mtrain_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_corpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embed_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36m_embed_documents\u001b[1;34m(self, train_corpus, batch_size)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             \u001b[0mdocument_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_l2_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdocument_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mlength_sorted_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_text_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[0msentences_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\topic_evaluate\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mlength_sorted_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_text_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[0msentences_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"incels\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                    #   custom_model=Top2VecNew,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"Top2Vec_incels_MiniLM_{i+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKxdYyJECpuY"
   },
   "source": [
    "# **DTM Evaluation**\n",
    "\n",
    "Here, we evaluate BERTopic and LDAseq on a dynamic topic modeling task with two datasets: \n",
    "* Trump's tweets\n",
    "* UN general debates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67jrSUR6DPbi"
   },
   "source": [
    "### **BERTopic**\n",
    "\n",
    "As seen before, we can load our data and generate embeddings before passing it to our evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl4xsRDgCtSY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"trump_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snszIP5bIbE4"
   },
   "source": [
    "Then, we also need to make sure that the timestamps match the data that were are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NhqOi6hqJDJ"
   },
   "outputs": [],
   "source": [
    "# Match indices\n",
    "import os\n",
    "os.listdir(f\"./{dataset}\")\n",
    "with open(f\"./{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Luc2euIfVL"
   },
   "source": [
    "Finally, we can simply run the Trainer as we did before but adding the timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqSmtipeIm8n"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    params = {\n",
    "        \"nr_topics\": [50],\n",
    "        \"min_topic_size\": 15,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"BERTopic\",\n",
    "                      params=params,\n",
    "                      bt_embeddings=embeddings,\n",
    "                      custom_dataset=custom,\n",
    "                      bt_timestamps=timestamps,\n",
    "                      topk=5,\n",
    "                      bt_nr_bins=10,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(f\"DynamicBERTopic_trump_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF4AFG2oDQn-"
   },
   "source": [
    "### **LDAseq**\n",
    "To run LDAseq, we again prepare our data and match the indices of our timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ4X6GymFYSu",
    "outputId": "22ab5c57-a279-48de-dc91-966c33a885c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119320it [03:25, 579.62it/s]\n",
      "278837it [00:00, 1751620.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(273743, 273743)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"un_dtm\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Match indices\n",
    "os.listdir(f\"{dataset}\")\n",
    "with open(f\"{dataset}/indexes.txt\") as f:\n",
    "    indices = f.readlines()\n",
    "    \n",
    "indices = [int(index.split(\"\\n\")[0]) for index in indices]\n",
    "indices_test = {index: True for index in indices}\n",
    "timestamps = [timestamp for index, timestamp in tqdm(enumerate(timestamps)) if indices_test.get(index)]\n",
    "len(data), len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAonxNdtJYwD"
   },
   "source": [
    "Then, we simply pass the timestamps and run our the trainer for LDAseq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wv_5-awJ4g7"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_topics\": [50],\n",
    "    \"nr_bins\": 9,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                  model_name=\"LDAseq\",\n",
    "                  params=params,\n",
    "                  custom_dataset=custom,\n",
    "                  bt_timestamps=timestamps,\n",
    "                  topk=5,\n",
    "                  verbose=True)\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNKfK8C3KH1M"
   },
   "source": [
    "We remove some information from the results as those are quite big to save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vB77Xa2qYPZ"
   },
   "outputs": [],
   "source": [
    "results[0][\"Params\"].keys()\n",
    "del results[0][\"Params\"][\"corpus\"]\n",
    "del results[0][\"Params\"][\"id2word\"]\n",
    "del results[0][\"Params\"][\"time_slice\"]\n",
    "\n",
    "import json\n",
    "with open(f\"LDAseq_trump.json\", 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-M13ABjKQYE"
   },
   "source": [
    "# **Wall time**\n",
    "Here, we only focus on the wall time of each topic model, from instantiating the model to training. To do so, we take the Trump dataset and split it up into steps of 1000 documents. Then, we can train a model and track the wall time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l0Yu9vSKfKc"
   },
   "outputs": [],
   "source": [
    "embedding_model = \"all-mpnet-base-v2\"\n",
    "# embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embedding_model_name = \"all-mpnet-base-v2\"\n",
    "topic_model_name = \"BERTopic_USE\"\n",
    "\n",
    "results = pd.DataFrame(columns=[\"dataset\", \"nr_documents\", \"vocab_size\", \"time\",\n",
    "                                \"cpu\", \"gpu\", \"gpu_cudnn\", \"gpu_memory\", \"embedding_model\"])\n",
    "for index, nr_documents in enumerate(tqdm(np.arange(1000, len(data), 2_000, dtype=int))):\n",
    "    \n",
    "    selected_data = random.sample(data, nr_documents)\n",
    "    selected_tokenized_data = random.sample(tokenized_data, nr_documents)\n",
    "    \n",
    "    if topic_model_name == \"CTM\":\n",
    "        qt, training_dataset = preprocess_ctm(selected_data, embedding_model_name)\n",
    "    \n",
    "    # Run model\n",
    "    start = time.time()\n",
    "    \n",
    "    if topic_model_name == \"LDA\":\n",
    "        id2word = corpora.Dictionary(selected_tokenized_data)\n",
    "        id_corpus = [id2word.doc2bow(document) for document in selected_tokenized_data]\n",
    "        lda = LdaMulticore(id_corpus, id2word=id2word, num_topics=100)\n",
    "    \n",
    "    elif topic_model_name == \"NFM\":\n",
    "        id2word = corpora.Dictionary(selected_tokenized_data)\n",
    "        id_corpus = [id2word.doc2bow(document) for document in selected_tokenized_data]\n",
    "        nmf_model = nmf.Nmf(id_corpus, id2word=id2word, num_topics=100)\n",
    "\n",
    "    elif topic_model_name == \"BERTopic\":\n",
    "        topic_model = BERTopic(embedding_model=embedding_model)    \n",
    "        topics, probs = topic_model.fit_transform(selected_data)\n",
    "        \n",
    "    elif topic_model_name == \"BERTopic_Doc2Vec\":\n",
    "        train_corpus = [TaggedDocument(default_tokenizer(doc), [i]) for i, doc in enumerate(selected_data)]\n",
    "        doc2vec_args = {\"vector_size\": 300,\n",
    "                        \"min_count\": 50,\n",
    "                        \"window\": 15,\n",
    "                        \"sample\": 1e-5,\n",
    "                        \"negative\": 0,\n",
    "                        \"hs\": 1,\n",
    "                        \"epochs\": 40,\n",
    "                        \"dm\": 0,\n",
    "                        \"dbow_words\": 1,\n",
    "                       \"documents\": train_corpus,\n",
    "                       \"workers\": -1}\n",
    "        model = Doc2Vec(**doc2vec_args)\n",
    "        embeddings = model.docvecs.vectors_docs\n",
    "        topic_model = BERTopic()    \n",
    "        topics, probs = topic_model.fit_transform(selected_data, embeddings)\n",
    "        \n",
    "    elif topic_model_name == \"BERTopic_USE\":\n",
    "        embeddings = embedding_model(selected_data).cpu().numpy()\n",
    "        topic_model = BERTopic(embedding_model=embedding_model)    \n",
    "        topics, probs = topic_model.fit_transform(selected_data, embeddings)\n",
    "\n",
    "    elif topic_model_name == \"Top2Vec\":\n",
    "        model = Top2Vec(selected_data, hdbscan_args={\"min_cluster_size\": 15}, workers=-1)\n",
    "#         model = Top2VecNew(selected_data, hdbscan_args={\"min_cluster_size\": 15}, embedding_model=embedding_model)\n",
    "        \n",
    "    elif topic_model_name == \"CTM\":\n",
    "        ctm = CombinedTM(n_components=100, contextual_size=768, bow_size=len(qt.vocab))\n",
    "        ctm.fit(training_dataset)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    # Calculate vocab size\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(selected_data)\n",
    "    vocab_size = len(vectorizer.get_feature_names())\n",
    "    \n",
    "    results.loc[len(results)] = [dataset, len(selected_data), vocab_size, end - start, cpu_name, gpu_name, \n",
    "                                 gpu_cudnn, gpu_memory, embedding_model_name]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERTopic OCTIS v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
